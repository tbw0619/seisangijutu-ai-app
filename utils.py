"""
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ç”»é¢è¡¨ç¤ºä»¥å¤–ã®æ§˜ã€…ãªé–¢æ•°å®šç¾©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚
"""

############################################################
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿
############################################################
import os
from dotenv import load_dotenv
import streamlit as st
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import HumanMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.text_splitter import CharacterTextSplitter
import constants as ct

# PDFå‡¦ç†ã¨ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®ãŸã‚ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from langchain_community.document_loaders import PyMuPDFLoader
    from langchain_community.vectorstores import FAISS
    VECTOR_SUPPORT = True
except ImportError as e:
    VECTOR_SUPPORT = False
    IMPORT_ERROR = str(e)


############################################################
# è¨­å®šé–¢é€£
############################################################
# ã€Œ.envã€ãƒ•ã‚¡ã‚¤ãƒ«ã§å®šç¾©ã—ãŸç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()


############################################################
# é–¢æ•°å®šç¾©
############################################################

def get_source_icon(source):
    """
    ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ä¸€ç·’ã«è¡¨ç¤ºã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡ã‚’å–å¾—

    Args:
        source: å‚ç…§å…ƒã®ã‚ã‚Šã‹

    Returns:
        ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ä¸€ç·’ã«è¡¨ç¤ºã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡
    """
    # å‚ç…§å…ƒãŒWebãƒšãƒ¼ã‚¸ã®å ´åˆã¨ãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã§ã€å–å¾—ã™ã‚‹ã‚¢ã‚¤ã‚³ãƒ³ã®ç¨®é¡ã‚’å¤‰ãˆã‚‹
    if source.startswith("http"):
        icon = ct.LINK_SOURCE_ICON
    else:
        icon = ct.DOC_SOURCE_ICON
    
    return icon


def build_error_message(message):
    """
    ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ç®¡ç†è€…å•ã„åˆã‚ã›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é€£çµ

    Args:
        message: ç”»é¢ä¸Šã«è¡¨ç¤ºã™ã‚‹ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

    Returns:
        ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ç®¡ç†è€…å•ã„åˆã‚ã›ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®é€£çµãƒ†ã‚­ã‚¹ãƒˆ
    """
    return "\n".join([message, ct.COMMON_ERROR_MESSAGE])


def get_llm_response(chat_message):
    """
    LLMã‹ã‚‰ã®å›ç­”å–å¾—

    Args:
        chat_message: ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å€¤

    Returns:
        LLMã‹ã‚‰ã®å›ç­”
    """
    # LLMã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”¨æ„
    llm = ChatOpenAI(model_name=ct.MODEL, temperature=ct.TEMPERATURE)

    # ä¼šè©±å±¥æ­´ãªã—ã§ã‚‚LLMã«ç†è§£ã—ã¦ã‚‚ã‚‰ãˆã‚‹ã€ç‹¬ç«‹ã—ãŸå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
    question_generator_template = ct.SYSTEM_PROMPT_CREATE_INDEPENDENT_TEXT
    question_generator_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_generator_template),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}")
        ]
    )

    # ãƒ¢ãƒ¼ãƒ‰ã«ã‚ˆã£ã¦LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¤‰æ›´
    if st.session_state.mode == ct.ANSWER_MODE_1:
        # ãƒ¢ãƒ¼ãƒ‰ãŒã€Œç¤¾å†…æ–‡æ›¸æ¤œç´¢ã€ã®å ´åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        question_answer_template = ct.SYSTEM_PROMPT_DOC_SEARCH
    else:
        # ãƒ¢ãƒ¼ãƒ‰ãŒã€Œç¤¾å†…å•ã„åˆã‚ã›ã€ã®å ´åˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        question_answer_template = ct.SYSTEM_PROMPT_INQUIRY
    # LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
    question_answer_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", question_answer_template),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}")
        ]
    )

    # ä¼šè©±å±¥æ­´ãªã—ã§ã‚‚LLMã«ç†è§£ã—ã¦ã‚‚ã‚‰ãˆã‚‹ã€ç‹¬ç«‹ã—ãŸå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã®Retrieverã‚’ä½œæˆ
    history_aware_retriever = create_history_aware_retriever(
        llm, st.session_state.retriever, question_generator_prompt
    )

    # LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®Chainã‚’ä½œæˆ
    question_answer_chain = create_stuff_documents_chain(llm, question_answer_prompt)
    # ã€ŒRAG x ä¼šè©±å±¥æ­´ã®è¨˜æ†¶æ©Ÿèƒ½ã€ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®Chainã‚’ä½œæˆ
    chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    # LLMã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—
    llm_response = chain.invoke({"input": chat_message, "chat_history": st.session_state.chat_history})
    # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¼šè©±å±¥æ­´ã«è¿½åŠ 
    st.session_state.chat_history.extend([HumanMessage(content=chat_message), llm_response["answer"]])

    return llm_response


def initialize_rag():
    """
    RAGæ©Ÿèƒ½ã®åˆæœŸåŒ–
    """
    if not VECTOR_SUPPORT:
        raise ImportError(f"å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“: {IMPORT_ERROR}")
    
    # å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’é¸æŠ
    existing_files = [pdf_path for pdf_path in ct.PDF_FILES if os.path.exists(pdf_path)]
    
    print(f"æ¤œç´¢å¯¾è±¡PDFãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(ct.PDF_FILES)}")
    print(f"å­˜åœ¨ã™ã‚‹PDFãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(existing_files)}")
    
    if not existing_files:
        raise ValueError("åˆ©ç”¨å¯èƒ½ãªPDFãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
    
    # OpenAI APIã‚­ãƒ¼ã®ç¢ºèª
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
    
    # å…¨PDFãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    all_documents = []
    for pdf_path in existing_files:
        try:
            print(f"èª­ã¿è¾¼ã¿ä¸­: {pdf_path}")
            loader = PyMuPDFLoader(pdf_path)
            documents = loader.load()
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¿½åŠ 
            for doc in documents:
                doc.metadata['source_file'] = os.path.basename(pdf_path)
            
            all_documents.extend(documents)
            print(f"  â†’ {len(documents)}ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å®Œäº†")
            
        except Exception as e:
            print(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {pdf_path}: {str(e)}")
            continue
    
    print(f"åˆè¨ˆãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(all_documents)}")
    
    if not all_documents:
        raise ValueError("PDFãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
    
    # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
    text_splitter = CharacterTextSplitter(
        chunk_size=ct.FAISS_CHUNK_SIZE,
        chunk_overlap=ct.FAISS_CHUNK_OVERLAP,
        separator="\n"
    )
    
    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ†å‰²
    split_docs = text_splitter.split_documents(all_documents)
    
    # ãƒãƒ£ãƒ³ã‚¯æ•°åˆ¶é™
    max_chunks = min(ct.FAISS_MAX_CHUNKS, len(split_docs))
    test_chunks = split_docs[:max_chunks]
    
    # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ã‚¿ãƒ¼ä½œæˆ
    embeddings = OpenAIEmbeddings(
        model=ct.OPENAI_EMBEDDING_MODEL
    )
    
    # FAISSãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ä½œæˆ
    vectorstore = FAISS.from_documents(test_chunks, embeddings)
    
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
    st.session_state.vectorstore = vectorstore
    st.session_state.pdf_chunks = test_chunks
    st.session_state.retriever = vectorstore.as_retriever(search_kwargs={"k": ct.SEARCH_K})
    
    # ä¼šè©±å±¥æ­´ã®åˆæœŸåŒ–
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    
    return True


def format_latex_equations(text):
    """
    LaTeXæ•°å¼ã®æ•´å½¢å‡¦ç†
    """
    import re
    
    # [ ] ã§å›²ã¾ã‚ŒãŸæ•°å¼ã‚’ $$ $$ ã«å¤‰æ›
    text = re.sub(r'\[\s*([^\[\]]+?)\s*\]', r'$$\1$$', text)
    
    # å˜ä¸€ã®$ã§å›²ã¾ã‚ŒãŸæ•°å¼ã‚’$$ã«å¤‰æ›ï¼ˆæ—¢ã«$$ã§å›²ã¾ã‚Œã¦ã„ãªã„å ´åˆã®ã¿ï¼‰
    text = re.sub(r'(?<!\$)\$(?!\$)([^$]+?)(?<!\$)\$(?!\$)', r'$$\1$$', text)
    
    # æ•°å¼å†…ã®ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’é©åˆ‡ã«å‡¦ç†ï¼ˆI_1 â†’ I_{1}ï¼‰
    text = re.sub(r'\$\$([^$]*?)([A-Za-z])_([0-9]+)([^$]*?)\$\$', r'$$\1\2_{\3}\4$$', text)
    
    # è¤‡æ•°ã®é€£ç¶šã™ã‚‹$$ã‚’æ•´ç†
    text = re.sub(r'\$\$\s*\$\$', r'$$', text)
    
    # æ•°å¼ã®å‰å¾Œã«é©åˆ‡ãªæ”¹è¡Œã¨ã‚¹ãƒšãƒ¼ã‚¹ã‚’è¿½åŠ 
    text = re.sub(r'\$\$([^$]+?)\$\$', r'\n\n$$\1$$\n\n', text)
    
    return text


def get_rag_chain_answer_qa(user_input):
    """
    RAGãƒã‚§ãƒ¼ãƒ³ã‚’ä½¿ã£ãŸå•ã„åˆã‚ã›å›ç­”ã®å–å¾—
    """
    if not st.session_state.get('rag_initialized', False) or st.session_state.get('retriever') is None:
        return {
            "answer": "RAGæ©Ÿèƒ½ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ã€ŒğŸš€ RAGæ©Ÿèƒ½ã‚’åˆæœŸåŒ–ã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚",
            "source_documents": []
        }
    
    try:
        # LLMã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç”¨æ„
        llm = ChatOpenAI(model_name=ct.MODEL, temperature=ct.TEMPERATURE)

        # ä¼šè©±å±¥æ­´ãªã—ã§ã‚‚LLMã«ç†è§£ã—ã¦ã‚‚ã‚‰ãˆã‚‹ã€ç‹¬ç«‹ã—ãŸå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½œæˆ
        question_generator_template = ct.SYSTEM_PROMPT_CREATE_INDEPENDENT_TEXT
        question_generator_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", question_generator_template),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}")
            ]
        )

        # å•ã„åˆã‚ã›ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        question_answer_template = ct.SYSTEM_PROMPT_INQUIRY
        question_answer_prompt = ChatPromptTemplate.from_messages(
            [
                ("system", question_answer_template),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}")
            ]
        )

        # ä¼šè©±å±¥æ­´ãªã—ã§ã‚‚LLMã«ç†è§£ã—ã¦ã‚‚ã‚‰ãˆã‚‹ã€ç‹¬ç«‹ã—ãŸå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹ãŸã‚ã®Retrieverã‚’ä½œæˆ
        history_aware_retriever = create_history_aware_retriever(
            llm, st.session_state.retriever, question_generator_prompt
        )

        # LLMã‹ã‚‰å›ç­”ã‚’å–å¾—ã™ã‚‹ç”¨ã®Chainã‚’ä½œæˆ
        question_answer_chain = create_stuff_documents_chain(llm, question_answer_prompt)
        # ã€ŒRAG x ä¼šè©±å±¥æ­´ã®è¨˜æ†¶æ©Ÿèƒ½ã€ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®Chainã‚’ä½œæˆ
        chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

        # LLMã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—
        chat_history = st.session_state.get('chat_history', [])
        llm_response = chain.invoke({"input": user_input, "chat_history": chat_history})
        
        # LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¼šè©±å±¥æ­´ã«è¿½åŠ 
        if "chat_history" not in st.session_state:
            st.session_state.chat_history = []
        st.session_state.chat_history.extend([HumanMessage(content=user_input), llm_response["answer"]])

        return {
            "answer": llm_response["answer"],
            "source_documents": llm_response.get("context", [])
        }
    
    except Exception as e:
        return {
            "answer": f"å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}",
            "source_documents": []
        }