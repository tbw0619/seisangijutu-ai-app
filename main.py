"""
çµ±åˆç‰ˆç”Ÿç”£æŠ€è¡“æˆæ¥­æ”¯æ´ã‚¢ãƒ—ãƒª
FAISS-RAGã¨æ—¢å­˜ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ§‹é€ ã‚’çµ±åˆ
"""

import streamlit as st
import os
import re
from dotenv import load_dotenv

# å†…éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import components
import constants as ct
import utils

# PDFå‡¦ç†ã¨ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®ãŸã‚ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from langchain_community.document_loaders import PyMuPDFLoader
    from langchain.text_splitter import CharacterTextSplitter
    from langchain_openai import OpenAIEmbeddings, ChatOpenAI
    from langchain_community.vectorstores import FAISS
    import numpy as np
    VECTOR_SUPPORT = True
except ImportError as e:
    VECTOR_SUPPORT = False
    IMPORT_ERROR = str(e)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title=f"{ct.APP_NAME}ï¼ˆçµ±åˆç‰ˆï¼‰", page_icon="ğŸ”§")

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []

if "rag_initialized" not in st.session_state:
    st.session_state.rag_initialized = False

if "vectorstore" not in st.session_state:
    st.session_state.vectorstore = None

if "pdf_chunks" not in st.session_state:
    st.session_state.pdf_chunks = []

if "mode" not in st.session_state:
    st.session_state.mode = ct.ANSWER_MODE_1


############################################################
# FAISS-RAGæ©Ÿèƒ½
############################################################

def load_pdf_with_faiss():
    """ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã•ã‚ŒãŸFAISS RAGåˆæœŸåŒ–"""
    from cost_optimizer import cost_optimizer, vector_manager
    
    try:
        # OpenAI APIã‚­ãƒ¼ã®ç¢ºèª
        if not os.getenv("OPENAI_API_KEY"):
            st.error("OpenAI APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
            return False
        
        # åŸ‹ã‚è¾¼ã¿ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        embeddings = OpenAIEmbeddings(
            model=ct.OPENAI_EMBEDDING_MODEL,
            show_progress_bar=True
        )
        
        # æ°¸ç¶šåŒ–ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
        if vector_manager.is_cache_valid():
            st.info("ğŸ”„ æ°¸ç¶šåŒ–ã•ã‚ŒãŸãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            vectorstore, cached_chunks = vector_manager.load_vector_store(embeddings)
            
            if vectorstore is not None and cached_chunks is not None:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å¾©å…ƒ
                st.session_state.vectorstore = vectorstore
                st.session_state.pdf_chunks = cached_chunks
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥åˆ†å¸ƒã‚’å†è¨ˆç®—
                file_distribution = {}
                for chunk in cached_chunks:
                    source = chunk.metadata.get('source_file', 'unknown')
                    file_distribution[source] = file_distribution.get(source, 0) + 1
                st.session_state.file_distribution = file_distribution
                
                st.success(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å¾©å…ƒ: {len(cached_chunks)}ãƒãƒ£ãƒ³ã‚¯ï¼ˆAPIä½¿ç”¨ãªã—ï¼‰")
                return True
        
        # æ–°è¦ä½œæˆã®å ´åˆ
        st.warning("ğŸ†• æ–°ã—ã„ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’ä½œæˆã—ã¾ã™ï¼ˆAPIä½¿ç”¨ï¼‰")
        
        # å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’é¸æŠ
        existing_files = [pdf_path for pdf_path in ct.PDF_FILES if os.path.exists(pdf_path)]
        
        if not existing_files:
            st.error("åˆ©ç”¨å¯èƒ½ãªPDFãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return False
        
        st.info(f"ğŸ“„ {len(existing_files)}å€‹ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        
        # å…¨PDFãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
        all_documents = []
        for i, pdf_path in enumerate(existing_files, 1):
            st.info(f"ğŸ“š PDFãƒ•ã‚¡ã‚¤ãƒ« {i}/{len(existing_files)} ã‚’èª­ã¿è¾¼ã¿ä¸­: {os.path.basename(pdf_path)}")
            
            try:
                loader = PyMuPDFLoader(pdf_path)
                documents = loader.load()
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ãƒ•ã‚¡ã‚¤ãƒ«åã‚’è¿½åŠ 
                for doc in documents:
                    doc.metadata['source_file'] = os.path.basename(pdf_path)
                
                all_documents.extend(documents)
                st.success(f"âœ… {os.path.basename(pdf_path)}: {len(documents)}ãƒšãƒ¼ã‚¸")
                
            except Exception as e:
                st.error(f"âŒ {os.path.basename(pdf_path)}ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
                continue
        
        if not all_documents:
            st.error("PDFãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            return False
        
        st.info(f"ğŸ“š åˆè¨ˆ {len(all_documents)} ãƒšãƒ¼ã‚¸ã‚’èª­ã¿è¾¼ã¿å®Œäº†")
        
        # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
        st.info("ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²ä¸­...")
        text_splitter = CharacterTextSplitter(
            chunk_size=ct.CHUNK_SIZE,
            chunk_overlap=ct.CHUNK_OVERLAP,
            separator="\n"
        )
        
        # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåˆ†å‰²
        split_docs = text_splitter.split_documents(all_documents)
        
        # ãƒãƒ£ãƒ³ã‚¯æ•°åˆ¶é™
        max_chunks = min(ct.MAX_CHUNKS, len(split_docs))
        test_chunks = split_docs[:max_chunks]
        st.info(f"ğŸ§ª {len(test_chunks)} ãƒãƒ£ãƒ³ã‚¯ã‚’ä½¿ç”¨ (å…¨{len(split_docs)}ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰)")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã®åˆ†å¸ƒã‚’è¡¨ç¤º
        file_distribution = {}
        for chunk in test_chunks:
            source = chunk.metadata.get('source_file', 'unknown')
            file_distribution[source] = file_distribution.get(source, 0) + 1
        
        # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ã‚¿ãƒ¼ä½œæˆï¼ˆAPIä½¿ç”¨ï¼‰
        st.info("ğŸ¤– åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ã‚¿ãƒ¼ã‚’ä½œæˆä¸­... (OpenAI APIä½¿ç”¨)")
        vectorstore = FAISS.from_documents(test_chunks, embeddings)
        
        # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’æ°¸ç¶šåŒ–ï¼ˆæ¬¡å›ã‹ã‚‰APIä¸è¦ï¼‰
        st.info("ğŸ’¾ ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’æ°¸ç¶šåŒ–ä¸­...")
        vector_manager.save_vector_store(vectorstore, test_chunks)
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
        st.session_state.vectorstore = vectorstore
        st.session_state.pdf_chunks = test_chunks
        st.session_state.file_distribution = file_distribution
        
        st.success(f"âœ… FAISS-RAGåˆæœŸåŒ–å®Œäº†: {len(test_chunks)}ãƒãƒ£ãƒ³ã‚¯ ({len(existing_files)}ãƒ•ã‚¡ã‚¤ãƒ«)")
        
        return True
        
    except Exception as e:
        st.error(f"FAISS-RAGåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False


def faiss_search(query, k=ct.FAISS_SEARCH_K):
    """FAISSæ¤œç´¢"""
    try:
        if st.session_state.vectorstore is None:
            return []
        
        # é¡ä¼¼åº¦æ¤œç´¢
        results = st.session_state.vectorstore.similarity_search_with_score(query, k=k)
        
        # çµæœã‚’æ•´å½¢
        formatted_results = []
        for doc, score in results:
            formatted_results.append({
                'content': doc.page_content,
                'metadata': doc.metadata,
                'similarity_score': score,
                'search_type': 'FAISS similarity'
            })
        
        return formatted_results
        
    except Exception as e:
        st.error(f"FAISSæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []


def clean_and_format_text(text):
    """æ•™ç§‘æ›¸ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿ã‚„ã™ãæ•´å½¢"""
    # æ”¹è¡Œã‚’é©åˆ‡ã«å‡¦ç†
    text = re.sub(r'\n+', '\n', text)  # è¤‡æ•°æ”¹è¡Œã‚’1ã¤ã«
    text = re.sub(r'\s+', ' ', text)   # è¤‡æ•°ã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«
    
    # ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤
    text = re.sub(r'[^\w\s\.\,\!\?\(\)\[\]\{\}\-\+\=\Ã—\Ã·\Â°\%\ï¼š\ï¼›\ã€\ã€‚\ï¼ˆ\ï¼‰\ã€Œ\ã€\ã€\ã€]', '', text)
    
    # å¥èª­ç‚¹ã®å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹ã‚’è¿½åŠ 
    text = re.sub(r'([ã€‚\.])\s*', r'\1 ', text)
    text = re.sub(r'([ã€\,])\s*', r'\1 ', text)
    
    # æ•°å¼ã‚’è¦‹ã‚„ã™ã
    text = re.sub(r'([A-Z])\s*=\s*', r'\n**\1 = ', text)
    text = re.sub(r'(\d+)\s*\.', r'\n\1. ', text)
    
    return text.strip()


def extract_key_points(content):
    """æ•™ç§‘æ›¸å†…å®¹ã‹ã‚‰é‡è¦ãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡º"""
    key_points = []
    
    # æ•°å¼ã‚’æŠ½å‡º
    formulas = re.findall(r'[A-Z]\s*=\s*[^ã€‚\n]+', content)
    for formula in formulas:
        key_points.append(f"ğŸ“ **å…¬å¼**: {formula.strip()}")
    
    # é‡è¦ãªæ¦‚å¿µã‚’æŠ½å‡º
    concepts = re.findall(r'([ã‚¢-ãƒ³]{2,}ã®æ³•å‰‡|[ã‚¢-ãƒ³]{2,}ã®å®šç†)', content)
    for concept in concepts:
        key_points.append(f"ğŸ”‘ **é‡è¦æ¦‚å¿µ**: {concept}")
    
    # å®šç¾©ã‚’æŠ½å‡º
    definitions = re.findall(r'([^ã€‚\n]*ã¨ã¯[^ã€‚\n]*)', content)
    for definition in definitions[:2]:  # æœ€å¤§2ã¤
        key_points.append(f"ğŸ’¡ **å®šç¾©**: {definition.strip()}")
    
    return key_points


def display_math_enhanced_response(response):
    """æ•°å¼è¡¨ç¤ºã‚’å¼·åŒ–ã—ãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹è¡¨ç¤º"""
    import re
    
    # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é †æ¬¡å‡¦ç†
    processed_response = response
    
    # ã¾ãš [ ] ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‡¦ç†
    bracket_pattern = r'\[\s*([^]]+)\s*\]'
    def replace_bracket_latex(match):
        formula = match.group(1)
        # LaTeXè¨˜æ³•ã‚’æ•´ç†
        clean_formula = (formula
                        .replace('\\times', 'Ã—')
                        .replace('\\text{A}', 'A')
                        .replace('\\text{V}', 'V')
                        .replace('\\text{Î©}', 'Î©')
                        .replace('\\text{W}', 'W')
                        .replace('\\text{', '')
                        .replace('}', '')
                        .replace('\\Omega', 'Î©')
                        .replace('\\,', ' ')
                        .replace('\\dots', 'â€¦')
                        .replace(',', '')
                        .replace('R2S', 'R2')  # R2Sã®ä¿®æ­£
                        .replace('\\', '')
                        .strip())
        return f"\n\n$${clean_formula}$$\n\n"
    
    processed_response = re.sub(bracket_pattern, replace_bracket_latex, processed_response)
    
    # $ $ ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‡¦ç†
    dollar_pattern = r'\$([^$]+)\$'
    def replace_dollar_latex(match):
        formula = match.group(1)
        clean_formula = (formula
                        .replace('\\times', 'Ã—')
                        .replace('\\text{A}', 'A')
                        .replace('\\text{V}', 'V')
                        .replace('\\text{Î©}', 'Î©')
                        .replace('\\text{W}', 'W')
                        .replace('\\text{', '')
                        .replace('}', '')
                        .replace('\\Omega', 'Î©')
                        .replace('\\,', ' ')
                        .replace('\\dots', 'â€¦')
                        .replace(',', '')
                        .replace('R2S', 'R2')  # R2Sã®ä¿®æ­£
                        .replace('\\', '')
                        .strip())
        return f"\n\n$${clean_formula}$$\n\n"
    
    processed_response = re.sub(dollar_pattern, replace_dollar_latex, processed_response)
    
    # $$ ã§å›²ã¾ã‚ŒãŸæ•°å¼ã‚’æ¤œå‡ºã—ã¦è¡¨ç¤º
    parts = re.split(r'\$\$([^$]+)\$\$', processed_response)
    
    for i, part in enumerate(parts):
        if i % 2 == 0:  # é€šå¸¸ã®ãƒ†ã‚­ã‚¹ãƒˆ
            if part.strip():
                st.markdown(part)
        else:  # LaTeXæ•°å¼
            try:
                # ã•ã‚‰ãªã‚‹æ¸…ç†
                latex_formula = (part.strip()
                               .replace('R2S', 'R2')
                               .replace('\\dots', '\\ldots')  # LaTeXç”¨ã®çœç•¥è¨˜å·
                               .replace('â€¦', '\\ldots'))
                
                # Streamlitã®st.latex()ã§è¡¨ç¤º
                st.latex(latex_formula)
                
                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆé–‹ç™ºæ™‚ã®ã¿ï¼‰
                with st.expander("ğŸ”§ æ•°å¼ãƒ‡ãƒãƒƒã‚°æƒ…å ±", expanded=False):
                    st.code(f"Original: {part}")
                    st.code(f"Cleaned: {latex_formula}")
                    
            except Exception as e:
                # LaTeXè¡¨ç¤ºã«å¤±æ•—ã—ãŸå ´åˆ
                try:
                    # ä»£æ›¿è¡¨ç¤ºæ–¹æ³•
                    clean_formula = (part.replace('Ã—', ' Ã— ')
                                   .replace('=', ' = ')
                                   .replace('R2S', 'R2')
                                   .replace('\\dots', 'â€¦')
                                   .replace('â€¦', ' â€¦ '))
                    st.markdown(f"### ğŸ“ æ•°å¼: `{clean_formula}`")
                    st.warning(f"LaTeXè¡¨ç¤ºã‚¨ãƒ©ãƒ¼: {e}")
                except:
                    # æœ€çµ‚çš„ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    st.markdown(f"**æ•°å¼**: {part}")
                    st.error("æ•°å¼ã®è¡¨ç¤ºã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ")
                # Streamlitã®st.latex()ã§è¡¨ç¤º
                st.latex(part.strip())
            except Exception as e:
                # LaTeXè¡¨ç¤ºã«å¤±æ•—ã—ãŸå ´åˆ
                try:
                    # ä»£æ›¿è¡¨ç¤ºæ–¹æ³•
                    clean_formula = part.replace('Ã—', ' Ã— ').replace('=', ' = ')
                    st.markdown(f"### ğŸ“ æ•°å¼: `{clean_formula}`")
                except:
                    # æœ€çµ‚çš„ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    st.markdown(f"**æ•°å¼**: {part}")


def enhance_math_display(text):
    """æ•°å¼è¡¨ç¤ºã‚’å¼·åŒ–ã™ã‚‹ãŸã‚ã®å¾Œå‡¦ç†"""
    import re
    
    # \text{A}ã€\text{V}ã€\text{Î©}ãªã©ã®LaTeXè¨˜æ³•ã‚’é€šå¸¸ã®æ–‡å­—ã«å¤‰æ›
    text = re.sub(r'\\text\{([^}]+)\}', r'\1', text)
    text = text.replace('\\,', ' ')  # LaTeXç©ºç™½ã‚’é€šå¸¸ç©ºç™½ã«
    text = text.replace('\\dots', 'â€¦')  # \dotsã‚’çœç•¥è¨˜å·ã«å¤‰æ›
    
    # ä¸€èˆ¬çš„ãªç‰©ç†å…¬å¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã—ã¦$è¨˜å·ã§å›²ã‚€
    math_patterns = [
        # åŸºæœ¬çš„ãªå…¬å¼
        (r'\bV\s*=\s*I\s*[Ã—*]\s*R\b', r'$V = I Ã— R$'),
        (r'\bP\s*=\s*V\s*[Ã—*]\s*I\b', r'$P = V Ã— I$'),
        (r'\bP\s*=\s*I\s*[Â²2]\s*[Ã—*]\s*R\b', r'$P = IÂ² Ã— R$'),
        (r'\bP\s*=\s*V\s*[Â²2]\s*/\s*R\b', r'$P = VÂ²/R$'),
        
        # æŠµæŠ—ã®æ¥ç¶šï¼ˆçœç•¥è¨˜å·ä»˜ãï¼‰
        (r'\bR\s*=\s*R1\s*\+\s*R2\s*\+\s*R3\s*\+\s*[â€¦\\dots]+\s*\+\s*Rn\b', r'$R = R1 + R2 + R3 + â€¦ + Rn$'),
        (r'\bR\s*=\s*R1\s*\+\s*R2S?\s*\+\s*R3\s*\+\s*[â€¦\\dots]+\s*\+\s*Rn\b', r'$R = R1 + R2 + R3 + â€¦ + Rn$'),
        (r'\bR\s*=\s*R1\s*\+\s*R2\b', r'$R = R1 + R2$'),
        (r'\b1/R\s*=\s*1/R1\s*\+\s*1/R2\s*\+\s*[â€¦\\dots]+\s*\+\s*1/Rn\b', r'$1/R = 1/R1 + 1/R2 + â€¦ + 1/Rn$'),
        (r'\b1/R\s*=\s*1/R1\s*\+\s*1/R2\b', r'$1/R = 1/R1 + 1/R2$'),
        
        # ã‚­ãƒ«ãƒ’ãƒ›ãƒƒãƒ•ã®æ³•å‰‡
        (r'\bÎ£V\s*=\s*0\b', r'$Î£V = 0$'),
        (r'\bÎ£I\s*=\s*0\b', r'$Î£I = 0$'),
        
        # æ•°å€¤ã‚’å«ã‚€è¨ˆç®—å¼ï¼ˆã‚ˆã‚Šè©³ç´°ã«ï¼‰
        (r'V\s*=\s*(\d+(?:\.\d+)?)\s*[,ï¼Œ]?\s*[A]\s*[Ã—*]\s*(\d+(?:\.\d+)?)\s*[,ï¼Œ]?\s*[Î©Î©]\s*=\s*(\d+(?:\.\d+)?)\s*[,ï¼Œ]?\s*[V]', r'$V = \1A Ã— \2Î© = \3V$'),
        (r'P\s*=\s*(\d+(?:\.\d+)?)\s*[,ï¼Œ]?\s*[V]\s*[Ã—*]\s*(\d+(?:\.\d+)?)\s*[,ï¼Œ]?\s*[A]\s*=\s*(\d+(?:\.\d+)?)\s*[,ï¼Œ]?\s*[W]', r'$P = \1V Ã— \2A = \3W$'),
    ]
    
    # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨
    for pattern, replacement in math_patterns:
        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
    
    # æ—¢å­˜ã®$è¨˜å·ãŒå«ã¾ã‚Œã¦ã„ã‚‹éƒ¨åˆ†ã®æ¸…ç†
    text = re.sub(r'\$([^$]*),\s*\\text\{([^}]+)\}([^$]*)\$', r'$\1\2\3$', text)
    
    # LaTeXè¨˜æ³•ã®å•é¡Œã‚’ä¿®æ­£
    text = re.sub(r'\$([^$]*R2S[^$]*)\$', lambda m: m.group(0).replace('R2S', 'R2'), text)
    text = re.sub(r'\$([^$]*\\dots[^$]*)\$', lambda m: m.group(0).replace('\\dots', 'â€¦'), text)
    
    # æ®‹ã£ã¦ã„ã‚‹LaTeXè¨˜æ³•ã‚’æ¸…ç†
    text = text.replace('\\times', 'Ã—')
    text = text.replace('\\Omega', 'Î©')
    
    return text
    text = text.replace('\\times', 'Ã—')
    text = text.replace('\\Omega', 'Î©')
    
    return text


def process_latex_in_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆå†…ã®LaTeXè¨˜æ³•ã‚’å‡¦ç†"""
    # æ§˜ã€…ãªLaTeXè¨˜æ³•ã‚’çµ±ä¸€
    text = re.sub(r'\[\s*([^]]+)\s*\]', r'$$\1$$', text)  # [ ] ã‚’ $$ $$ ã«å¤‰æ›
    text = re.sub(r'\$([^$]+)\$', r'$$\1$$', text)        # $ $ ã‚’ $$ $$ ã«å¤‰æ›
    
    # LaTeXè¨˜æ³•ã‚’æ•´ç†
    text = text.replace('\\times', 'Ã—')
    text = text.replace('\\text{', '')
    text = text.replace('}', '')
    text = text.replace('\\Omega', 'Î©')
    text = text.replace('\\mathrm{', '')
    text = text.replace('\\,', ' ')
    
    return text


def safe_latex_format(text):
    """LaTeXæ•°å¼ã‚’å®‰å…¨ãªå½¢å¼ã«å¤‰æ›"""
    # è¤‡é›‘ãªLaTeXè¨˜æ³•ã‚’ç°¡ç´ åŒ–
    text = text.replace('\\times', 'Ã—')
    text = text.replace('\\frac{', '(')
    text = text.replace('}{', ')/(')
    text = text.replace('}', ')')
    text = text.replace('\\sum', 'Î£')
    text = text.replace('^2', 'Â²')
    text = text.replace('^3', 'Â³')
    text = text.replace('\\', '')
    return text


def generate_openai_student_answer(query, context_text):
    """ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã•ã‚ŒãŸOpenAI APIå›ç­”ç”Ÿæˆ"""
    from cost_optimizer import cost_optimizer
    
    try:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå›ç­”ã‚’ãƒã‚§ãƒƒã‚¯
        cached_response = cost_optimizer.get_cached_response(query)
        if cached_response:
            st.info("ğŸ’° ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰å›ç­”ã‚’å–å¾—ï¼ˆAPIä½¿ç”¨ãªã—ï¼‰")
            return cached_response
        
        # æ—¥æ¬¡åˆ¶é™ã‚’ãƒã‚§ãƒƒã‚¯
        if not cost_optimizer.check_daily_limit():
            return "æœ¬æ—¥ã®APIä½¿ç”¨åˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå›ç­”ã®ã¿åˆ©ç”¨å¯èƒ½ã§ã™ã€‚"
        
        # å…¥åŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¤œè¨¼
        if not query:
            query = "è³ªå•å†…å®¹ãªã—"
        if not context_text:
            context_text = "é–¢é€£ã™ã‚‹æ•™ç§‘æ›¸ã®å†…å®¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        # OpenAI ChatGPTãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ï¼ˆã‚³ã‚¹ãƒˆæœ€é©åŒ–æ¸ˆã¿ï¼‰
        llm = ChatOpenAI(
            model=ct.OPENAI_CHAT_MODEL,  # gpt-4o-miniã«å¤‰æ›´æ¸ˆã¿
            temperature=ct.OPENAI_TEMPERATURE,
            max_tokens=ct.OPENAI_MAX_TOKENS  # 1500ã«å‰Šæ¸›æ¸ˆã¿
        )
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆï¼ˆLaTeXè¨˜æ³•ã‚’é¿ã‘ã¦ã‚·ãƒ³ãƒ—ãƒ«ã«ï¼‰
        try:
            prompt = ct.SYSTEM_PROMPT_STUDENT_FRIENDLY.format(
                query=query,
                context=context_text
            )
        except Exception as format_error:
            st.error(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚¨ãƒ©ãƒ¼: {format_error}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ç°¡å˜ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            prompt = f"""å·¥æ¥­é«˜æ ¡ç”Ÿå‘ã‘ã«åˆ†ã‹ã‚Šã‚„ã™ãå›ç­”ã—ã¦ãã ã•ã„ã€‚
            
è³ªå•: {query}

æ•™ç§‘æ›¸ã®å†…å®¹: {context_text}

æ•°å¼ã¯$è¨˜å·ã§å›²ã‚“ã§è¡¨ç¤ºã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼š$V = I Ã— R$ï¼‰ã€‚"""
        
        # APIä½¿ç”¨é‡ã‚’ã‚¤ãƒ³ã‚¯ãƒªãƒ¡ãƒ³ãƒˆ
        cost_optimizer.increment_usage()
        
        # OpenAI APIã§å›ç­”ç”Ÿæˆ
        st.info("ğŸ¤– GPT-4o-miniã§å›ç­”ç”Ÿæˆä¸­...")
        response = llm.invoke(prompt)
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        cost_optimizer.cache_response(query, response.content)
        
        return response.content
        
    except Exception as e:
        error_message = str(e)
        st.error(f"OpenAI API ã‚¨ãƒ©ãƒ¼: {error_message}")
        
        # ã‚¨ãƒ©ãƒ¼ã®ç¨®é¡ã«å¿œã˜ã¦å¯¾å‡¦æ³•ã‚’æç¤º
        if "rate_limit" in error_message.lower():
            st.warning("APIåˆ©ç”¨åˆ¶é™ã«é”ã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚")
        elif "invalid_request" in error_message.lower():
            st.warning("ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç°¡ç•¥åŒ–ã—ã¦å†è©¦è¡Œã—ã¾ã™ã€‚")
        elif "authentication" in error_message.lower():
            st.error("OpenAI APIã‚­ãƒ¼ã®èªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸã€‚è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”
        return f"""ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ãŒã€å›ç­”ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚

**ã‚¨ãƒ©ãƒ¼è©³ç´°**: {error_message}

**æ•™ç§‘æ›¸ã®é–¢é€£å†…å®¹**:
{context_text[:500] if context_text else "å†…å®¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"}...

æ•™ç§‘æ›¸ã®å†…å®¹ã‚’ã‚‚ã¨ã«ã€æ‰‹å‹•ã§å›ç­”ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"""


def generate_faiss_response(query, search_results, mode):
    """FAISSæ¤œç´¢çµæœã‹ã‚‰å¿œç­”ç”Ÿæˆ"""
    if not search_results:
        return "é–¢é€£ã™ã‚‹æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚è³ªå•ã‚’å¤‰ãˆã¦ã¿ã¦ãã ã•ã„ã€‚"
    
    if mode == ct.ANSWER_MODE_1:  # æ•™ç§‘æ›¸æ¤œç´¢
        response = f"## ğŸ“šã€Œ{query}ã€ã«é–¢é€£ã™ã‚‹æ•™ç§‘æ›¸ã®å†…å®¹\n\n"
        
        for i, result in enumerate(search_results, 1):
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿ã‚„ã™ãæ•´å½¢
            cleaned_content = clean_and_format_text(result['content'])
            
            # é‡è¦ãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡º
            key_points = extract_key_points(result['content'])
            
            score = result['similarity_score']
            source_file = result['metadata'].get('source_file', 'unknown')
            
            response += f"### ğŸ“– æ¤œç´¢çµæœ {i} (é¡ä¼¼åº¦: {score:.3f})\n"
            response += f"**å‡ºå…¸**: {source_file}\n\n"
            
            # é‡è¦ãƒã‚¤ãƒ³ãƒˆãŒã‚ã‚Œã°æœ€åˆã«è¡¨ç¤º
            if key_points:
                response += "**é‡è¦ãƒã‚¤ãƒ³ãƒˆ**:\n"
                for point in key_points[:3]:  # æœ€å¤§3ã¤
                    response += f"- {point}\n"
                response += "\n"
            
            # æ•´å½¢ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
            if len(cleaned_content) > 400:
                response += f"**å†…å®¹**: {cleaned_content[:400]}...\n\n"
            else:
                response += f"**å†…å®¹**: {cleaned_content}\n\n"
            
            response += "---\n\n"
        
        return response
    
    else:  # å•ã„åˆã‚ã›ãƒ¢ãƒ¼ãƒ‰
        # æ¤œç´¢çµæœã‹ã‚‰é–¢é€£ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
        context_content = []
        for result in search_results[:3]:  # æœ€å¤§3ã¤ã®çµæœã‚’ä½¿ç”¨
            cleaned_content = clean_and_format_text(result['content'])
            source_file = result['metadata'].get('source_file', 'unknown')
            context_content.append(f"ã€å‡ºå…¸: {source_file}ã€‘\n{cleaned_content}")
        
        # æ•™ç§‘æ›¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’çµåˆ
        context_text = "\n\n".join(context_content)
        
        # OpenAI APIã‚’ä½¿ã£ã¦å·¥æ¥­é«˜æ ¡ç”Ÿå‘ã‘ã®å›ç­”ã‚’ç”Ÿæˆ
        answer = generate_openai_student_answer(query, context_text)
        
        # æ•°å¼è¡¨ç¤ºã‚’å¼·åŒ–ã™ã‚‹ãŸã‚ã®å¾Œå‡¦ç†
        answer = enhance_math_display(answer)
        
        # è¿½åŠ ã®æ•°å¼ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        if '=' in answer and ('V' in answer or 'I' in answer or 'R' in answer or 'P' in answer):
            # æ•°å¼ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒé«˜ã„å ´åˆã¯ã€ã•ã‚‰ã«å‡¦ç†
            answer = re.sub(r'([VIRPvipr])\s*=\s*([^$\n]+?)(?=\n|$)', 
                          lambda m: f"${m.group(1)} = {m.group(2).strip()}$" if '$' not in m.group(0) else m.group(0), 
                          answer)
        
        # å‚è€ƒæƒ…å ±ã‚’è¿½åŠ 
        answer += "\n\n---\n\n**ğŸ“š å‚è€ƒã«ã—ãŸæ•™ç§‘æ›¸ã®å†…å®¹**:\n"
        for i, result in enumerate(search_results[:2], 1):
            source_file = result['metadata'].get('source_file', 'unknown')
            score = result['similarity_score']
            preview = clean_and_format_text(result['content'])[:150] + "..."
            answer += f"\n{i}. **{source_file}** (é–¢é€£åº¦: {score:.1f})\n{preview}\n"
        
        return answer


############################################################
# ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
############################################################

def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³"""
    
    # ã‚¿ã‚¤ãƒˆãƒ«è¡¨ç¤º
    components.display_app_title()
    st.markdown("**çµ±åˆç‰ˆ - FAISSé«˜ç²¾åº¦æ¤œç´¢å¯¾å¿œ**")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("åˆ©ç”¨ç›®çš„")
        
        # ãƒ¢ãƒ¼ãƒ‰é¸æŠ
        components.display_select_mode()
        
        # ãƒ¢ãƒ¼ãƒ‰åˆ¥èª¬æ˜
        if st.session_state.mode == ct.ANSWER_MODE_1:
            st.info("FAISSæ¤œç´¢ã§é«˜ç²¾åº¦ãªæ•™ç§‘æ›¸å†…å®¹æ¤œç´¢ãŒã§ãã¾ã™ã€‚é‡è¦ãƒã‚¤ãƒ³ãƒˆã‚’æ•´ç†ã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚")
            st.code("ã‚­ãƒ«ãƒ’ãƒ›ãƒƒãƒ•ã®æ³•å‰‡ã«ã¤ã„ã¦", language=None)
        else:
            st.info("å·¥æ¥­é«˜æ ¡ç”Ÿå‘ã‘ã®åˆ†ã‹ã‚Šã‚„ã™ã„å›ç­”ã‚’GPT-4o-miniã§æä¾›ã—ã¾ã™ã€‚è¨ˆç®—å•é¡Œã‚‚è©³ç´°ã«è§£èª¬ã—ã¾ã™ã€‚")
            st.code("ã‚­ãƒ«ãƒ’ãƒ›ãƒƒãƒ•ã®æ³•å‰‡ã‚’åˆ†ã‹ã‚Šã‚„ã™ãæ•™ãˆã¦", language=None)
            st.code("ã‚ªãƒ¼ãƒ ã®æ³•å‰‡ã§é›»æµ2Aã€æŠµæŠ—5Î©ã®æ™‚ã®é›»åœ§ã¯ï¼Ÿ", language=None)
            st.code("ç›´åˆ—å›è·¯ã®åˆæˆæŠµæŠ—ã®è¨ˆç®—æ–¹æ³•ã¯ï¼Ÿ", language=None)
        
        # ã‚³ã‚¹ãƒˆç®¡ç†ãƒ‘ãƒãƒ«
        st.markdown("---")
        st.markdown("**ğŸ’° ã‚³ã‚¹ãƒˆç®¡ç†**")
        
        from cost_optimizer import cost_optimizer, vector_manager
        
        # ä½¿ç”¨çµ±è¨ˆã®è¡¨ç¤º
        usage_stats = cost_optimizer.get_usage_stats()
        col1, col2 = st.columns(2)
        with col1:
            st.metric("æœ¬æ—¥ã®ä½¿ç”¨", f"{usage_stats['today_calls']}")
        with col2:
            st.metric("æ®‹ã‚Šå›æ•°", f"{usage_stats['remaining_calls']}")
        
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
        progress = usage_stats['today_calls'] / ct.MAX_DAILY_API_CALLS
        st.progress(progress, text=f"æ—¥æ¬¡åˆ¶é™: {usage_stats['today_calls']}/{ct.MAX_DAILY_API_CALLS}")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
        st.markdown("**ğŸ—„ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†**")
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ§¹ å¿œç­”ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"):
                cost_optimizer.clean_old_cache()
                st.success("å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤ã—ã¾ã—ãŸ")
        with col2:
            if st.button("ğŸ”„ ãƒ™ã‚¯ã‚¿ãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢"):
                vector_manager.clear_cache()
                st.session_state.rag_initialized = False
                st.rerun()
        
        # FAISS-RAGåˆæœŸåŒ–
        st.markdown("---")
        st.markdown("**ğŸ§  RAGæ©Ÿèƒ½ã®åˆæœŸåŒ–**")
        
        if components.display_faiss_initialization_sidebar():
            with st.spinner(ct.SPINNER_TEXT):
                success = load_pdf_with_faiss()
                if success:
                    st.session_state.rag_initialized = True
                    st.success("FAISS-RAGæ©Ÿèƒ½ã®åˆæœŸåŒ–ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
                    st.rerun()
                else:
                    st.session_state.rag_initialized = False
    
    # FAISS-RAGæ©Ÿèƒ½ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
    components.display_faiss_rag_status()
    
    # åˆæœŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¿½åŠ ï¼ˆåˆå›ã®ã¿ï¼‰
    if not st.session_state.messages:
        components.display_initial_ai_message()
    
    # ä¼šè©±å±¥æ­´è¡¨ç¤º
    components.display_conversation_log()
    
    # ãƒãƒ£ãƒƒãƒˆå…¥åŠ›
    if st.session_state.rag_initialized:
        if prompt := st.chat_input(ct.CHAT_INPUT_HELPER_TEXT):
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤º
            with st.chat_message("user"):
                st.write(prompt)
            st.session_state.messages.append({"role": "user", "content": prompt})
            
            # AIå¿œç­”ã‚’ç”Ÿæˆ
            with st.chat_message("assistant"):
                with st.spinner(ct.SPINNER_TEXT):
                    # FAISSæ¤œç´¢å®Ÿè¡Œ
                    search_results = faiss_search(prompt, k=ct.FAISS_SEARCH_K)
                    
                    # å¿œç­”ç”Ÿæˆ
                    response = generate_faiss_response(prompt, search_results, st.session_state.mode)
                    
                    # å¿œç­”å½¢å¼ã«å¿œã˜ã¦è¡¨ç¤º
                    if st.session_state.mode == ct.ANSWER_MODE_1:
                        # æ•™ç§‘æ›¸æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ï¼šãã®ã¾ã¾è¡¨ç¤º
                        st.write(response)
                        content = {
                            "mode": ct.ANSWER_MODE_1,
                            "answer": response
                        }
                    else:
                        # å•ã„åˆã‚ã›ãƒ¢ãƒ¼ãƒ‰ï¼šæ•°å¼å¼·åŒ–è¡¨ç¤º
                        with st.expander("ğŸ”§ æ•°å¼å‡¦ç†æƒ…å ±ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰", expanded=False):
                            st.text("å…ƒã®å›ç­”:")
                            st.text(response[:200] + "..." if len(response) > 200 else response)
                            
                            # æ•°å¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡ºçŠ¶æ³
                            math_found = []
                            if '$' in response:
                                math_found.append("$è¨˜å·ã‚ã‚Š")
                            if 'V = I' in response:
                                math_found.append("ã‚ªãƒ¼ãƒ ã®æ³•å‰‡")
                            if 'P = V' in response:
                                math_found.append("é›»åŠ›å…¬å¼")
                            
                            st.text(f"æ¤œå‡ºã•ã‚ŒãŸæ•°å¼ãƒ‘ã‚¿ãƒ¼ãƒ³: {', '.join(math_found) if math_found else 'ãªã—'}")
                        
                        display_math_enhanced_response(response)
                        content = {
                            "mode": ct.ANSWER_MODE_2,
                            "answer": response
                        }
                    
                    # æ¤œç´¢çµæœè©³ç´°è¡¨ç¤º
                    components.display_faiss_search_results(search_results)
            
            # ä¼šè©±ãƒ­ã‚°ã«è¿½åŠ 
            st.session_state.messages.append({
                "role": "assistant", 
                "content": content
            })
    
    else:
        st.info("âš¡ FAISS-RAGæ©Ÿèƒ½ã‚’åˆæœŸåŒ–ã—ã¦ã‹ã‚‰è³ªå•ã‚’é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")
    
    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
    st.markdown("---")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("ã‚¢ãƒ—ãƒªçŠ¶æ…‹", "âœ… æ­£å¸¸" if VECTOR_SUPPORT else "âŒ ã‚¨ãƒ©ãƒ¼")
    
    with col2:
        rag_status = "âœ… æœ‰åŠ¹" if st.session_state.rag_initialized else "âš ï¸ ç„¡åŠ¹"
        st.metric("FAISS-RAG", rag_status)
    
    with col3:
        chunk_count = len(st.session_state.pdf_chunks) if st.session_state.pdf_chunks else 0
        st.metric("ãƒ™ã‚¯ã‚¿ãƒ¼æ•°", f"{chunk_count}ä»¶")
    
    # çµ±åˆç‰ˆã®èª¬æ˜
    with st.expander("â„¹ï¸ çµ±åˆç‰ˆã«ã¤ã„ã¦"):
        st.markdown(f"""
        **ã‚³ã‚¹ãƒˆæœ€é©åŒ–ç‰ˆã®ç‰¹å¾´:**
        - **ğŸ¦ æ°¸ç¶šåŒ–ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ï¼ˆå†embeddingãªã—ï¼‰
        - **ğŸ’¾ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥**: åŒã˜è³ªå•ã®å›ç­”ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆ24æ™‚é–“ï¼‰
        - **ğŸ“Š ä½¿ç”¨é‡åˆ¶é™**: æ—¥æ¬¡APIå‘¼ã³å‡ºã—åˆ¶é™ï¼ˆ{ct.MAX_DAILY_API_CALLS}å›/æ—¥ï¼‰
        - **ğŸ¤– è»½é‡ãƒ¢ãƒ‡ãƒ«**: GPT-4o-miniã§ã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼ˆå¾“æ¥ã®1/10ã®æ–™é‡‘ï¼‰
        - **âš¡ é«˜é€Ÿæ¤œç´¢**: FAISSæ„å‘³çš„é¡ä¼¼åº¦æ¤œç´¢
        - **ğŸ“ LaTeXæ•°å¼**: ç¾ã—ã„æ•°å¼ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
        
        **ã‚³ã‚¹ãƒˆå‰Šæ¸›åŠ¹æœ:**
        - åˆå›ã®ã¿embedding APIä½¿ç”¨ï¼ˆæ¬¡å›ã‹ã‚‰ãƒ­ãƒ¼ã‚«ãƒ«èª­ã¿è¾¼ã¿ï¼‰
        - GPT-4o â†’ GPT-4o-miniï¼ˆç´„90%ã‚³ã‚¹ãƒˆå‰Šæ¸›ï¼‰
        - ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã§é‡è¤‡è³ªå•ã®APIä½¿ç”¨ã‚¼ãƒ­
        - æ—¥æ¬¡åˆ¶é™ã§äºˆç®—ç®¡ç†
        
        **è¨ˆç®—æ©Ÿèƒ½:**
        - ã‚ªãƒ¼ãƒ ã®æ³•å‰‡è¨ˆç®—
        - ã‚­ãƒ«ãƒ’ãƒ›ãƒƒãƒ•ã®æ³•å‰‡
        - é›»åŠ›è¨ˆç®—
        - æŠµæŠ—ã®ç›´åˆ—ãƒ»ä¸¦åˆ—æ¥ç¶š
        - æ•°å¼ã®LaTeXè¡¨ç¤º
        
        **PDFãƒ•ã‚¡ã‚¤ãƒ«:**
        - å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(ct.PDF_FILES)}
        - ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {ct.FAISS_CHUNK_SIZE}
        - æœ€å¤§ãƒãƒ£ãƒ³ã‚¯æ•°: {ct.FAISS_MAX_CHUNKS}
        """)


if __name__ == "__main__":
    main()
